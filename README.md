# Proposal: RAG Chatbot for Trading Product Knowledge

## Goal and Scope

The goal is to develop a **Retrieval-Augmented Generation (RAG)** chatbot that can serve as a knowledgeable assistant for trading-related product information. The chatbot will retrieve relevant facts from internal content and generate accurate answers for users (support staff or customers). Key data sources to be integrated include:

* **Product Manuals (PDF):** Detailed guides and technical manuals for trading products.
* **Selected Web Pages & Blog Posts (PDF):** Educational or marketing content in PDF form.
* **Trading Manifesto (Text/PDF):** A document outlining trading principles and guidelines.
* **Educational Video Transcripts (Text):** Transcripts of training videos (timestamps removed).
* **Zendesk Knowledge Base Articles (JSON via API):** Existing support Q\&A and how-to articles.


## Constraints and Considerations

* **Automated Ingestion:** The solution must automatically parse files and chunk content intelligently. Tools that provide *partitioning, chunking, and embedding out-of-the-box* are preferred.
* **Overlapping Content:** Different sources may contain duplicate or overlapping information. Each source will be ingested independently (no manual deduplication). The retrieval mechanism should handle potential duplicates (e.g. by filtering by source or merging results).
* **Data Freshness:** New documents (e.g. new manuals or articles) should be easy to add. The pipeline should allow re-indexing or incremental updates without extensive rework.
* **Minimal Development Effort:** We prefer using well-supported open-source libraries or managed services to avoid “reinventing the wheel.” The goal is to minimize custom code for parsing or vectorizing text. This reduces both initial development time and long-term maintenance points of failure.
* **Runtime Efficiency:** The chosen stack should be cost-effective and performant. We seek to avoid extremely large infrastructure costs (e.g. running a very large model 24/7 unnecessarily) and also avoid high per-query costs if usage grows. We will compare self-hosting vs. pay-per-use to strike the right balance.
* **Scalability and Reliability:** The system should scale to a growing knowledge base and more concurrent users. Managed components that auto-scale or proven open-source tools that support clustering will be considered. We also want to minimize single points of failure – e.g. if an embedding service or model goes down, it should fail gracefully or have a fallback.

## Architecture Options

To implement the RAG chatbot, we break the architecture into key components and evaluate options for each:

&#x20;*High-level RAG architecture: The user’s query is processed by a retrieval module that fetches relevant data from internal sources (PDFs, knowledge base, etc.), which is then provided as context to the Generation Model (LLM) to produce a response.*

### Vector Database (Knowledge Store)

The vector database stores embeddings of all documents and allows fast similarity search for relevant chunks. We consider:

* **Open-Source (Self-Hosted) Vector DB:** Examples include **Qdrant**, **Weaviate**, **Milvus**, or **Chroma**. These can be deployed on our own infrastructure (Docker or Kubernetes). They are free to use and give full control over data and deployment topology. Qdrant, for instance, can be run with a single Docker command and even offers a managed cloud, but the open-source version has no license cost. Self-hosted solutions allow flexible **deployment (on-prem or cloud)** and avoid vendor lock-in. However, we must handle operations: scaling, upgrades, and ensuring high availability are our responsibility. This requires DevOps effort especially if we anticipate large scale (billions of vectors) or high query throughput.
* **Managed Vector DB as a Service:** e.g. **Pinecone**. Pinecone is a fully managed SaaS vector database with a simple API. It abstracts away all infrastructure management – scaling, replication, hardware – in exchange for a usage-based cost. Pinecone has a **free tier** (approx. 300k vectors at 1536 dimensions) and paid plans thereafter. The managed route offers ease of use (just an API endpoint) and guaranteed performance, with features like automatic scaling and security (encryption, RBAC) out-of-the-box. The trade-off is **cost** and **control**: it can become expensive at scale and data resides in a third-party cloud (though Pinecone is SOC2 compliant). Also, Pinecone is proprietary (cannot be self-hosted), meaning potential lock-in and less flexibility for custom vector filtering logic.

**Vector DB Option Comparison:** The table below summarizes the pros and cons of self-hosted vs. managed vector stores for our use case.

| Option                                                       | Pros                                                                                                                                                                                                                 | Cons                                                                                                                                                                                                                                                     |
| ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Self-Hosted Vector DB**<br>(e.g. Qdrant, Weaviate, Milvus) | • Full control over deployment and data (on-prem or private cloud)<br>• Open-source (no license fees) – cost is only for infrastructure we run<br>• Flexible customization (advanced filtering, hybrid search, etc.) | • Requires our team to deploy, monitor, and scale the database (DevOps overhead)<br>• Must provision servers for peak load (underutilized resources if load is low)<br>• Upgrades and maintenance are our responsibility (higher operational complexity) |
| **Managed Vector DB Service**<br>(e.g. Pinecone)             | • Zero infrastructure to manage – handles scaling, replication, backups<br>• Fast setup and integration via simple API<br>• Built-in security and compliance (encryption, RBAC, SOC2)                                | • Usage-based cost can grow with data size & queries (recurring fees)<br>• Data stored with third-party (potential privacy concern)<br>• Vendor lock-in risk; not self-hostable (dependent on Pinecone features and pricing)                             |

Given our moderate team size and need to minimize complexity, we will likely favor an open-source vector DB initially (for cost control and data ownership) – especially if our data volume (number of chunks) fits comfortably in a single-instance setup. Qdrant is a strong candidate due to its performance and simple setup. Weaviate or Milvus are also options; all have Python client libraries and integrations with popular frameworks. If down the line the operational burden becomes too high, we can re-evaluate Pinecone or another managed service.

### LLM (Language Model) Options

For the generative model that will take the retrieved context and produce answers, we have two broad choices:

* **Self-Hosted Open-Source LLM:** Models like **Meta LLaMA 2 (or 3)** or **Mistral 7B/13B** can be run on our own hardware. New open models are rapidly improving – the best open models already approach or exceed GPT-3.5 on many benchmarks. Self-hosting an LLM ensures **data privacy** (no query or context ever leaves our environment), and we incur **no per-query fees** (we pay for the machine time whether it’s used or idle). We can also customize the model (through fine-tuning or system prompts) without vendor restrictions. However, hosting LLMs has challenges:

  * **Infrastructure**: Large models require GPUs or specialized hardware. For example, a 13B parameter model can run on a single GPU, but a 70B model might require multiple high-memory GPUs. This can mean substantial upfront cost or cloud GPU rental (\~\$3–\$5 per hour for a 80GB A100 GPU, equating to \$2k–\$4k per month). For smaller models like 7B or 13B, we might use an existing server with an A10 or even CPU (with performance trade-offs).
  * **Expertise**: We need some ML ops know-how to optimize model serving (loading, quantizing models, etc.) and to maintain it. Self-hosted LLMs “demand an existing infrastructure team and proper understanding” of the model’s needs.
  * **Performance**: The quality of open models, while improving, may still lag behind the very latest proprietary models (e.g. GPT-4). There is a risk that answers might be less fluent or accurate in complex cases. We can mitigate this by carefully evaluating open models on our domain (trading Q\&A) – often a smaller fine-tuned model can perform well on a narrow domain.
* **Hosted LLM API:** This includes services like **OpenAI’s GPT-3.5/GPT-4** or **Groq** (which hosts optimized open models on cloud hardware). The benefits are **easy integration (fast go-to-market)** – we simply call an API, with no need to manage hardware or model updates. These models are state-of-the-art in quality out-of-the-box. We also get reliability and scalability handled by the provider (if we suddenly have a spike in queries, the API scales on its side). The downsides:

  * **Cost per query:** Usage is pay-as-you-go. For example, GPT-4 is around \$0.03 per 1K input tokens and \$0.06 per 1K output tokens. While seemingly low, this can accumulate with heavy usage. We effectively pay every time the model runs, which could be costly for high volume. The flip side is **no idle cost** – if usage is low, an API might be cheaper than running our own server 24/7.
  * **Data Privacy & Compliance:** Using an external API means sending our proprietary context (e.g. manual excerpts) to a third-party server. OpenAI provides guarantees (data not used for training, etc.), but this may be a compliance issue if the content is highly sensitive. For internal-facing use, this may be acceptable; for customer-facing, we’d review privacy policies. Self-hosting avoids this concern entirely.
  * **Limited customization:** We can’t fine-tune OpenAI’s base models on our data (unless using their fine-tuning service for GPT-3.5). We rely on prompt engineering alone. In contrast, with an open model we could, for instance, fine-tune on our Q\&A pairs or trading jargon if needed. That said, RAG itself reduces the need for fine-tuning by injecting relevant text.

**LLM Option Comparison:** The table below summarizes the trade-offs between self-hosting an LLM and using a hosted LLM API.

| Option **(LLM)**                                                    | Pros                                                                                                                                                                                                                    | Cons                                                                                                                                                                                                                          |
| ------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Self-Hosted Open-Source**<br>(e.g. LLaMA 2/3, Mistral via Ollama) | • Data never leaves our infrastructure (max privacy control)<br>• No per-request fees – fixed infrastructure cost can be more economical at scale<br>• Full flexibility to fine-tune or modify the model behavior       | • Requires GPU hardware and ML ops expertise to deploy<br>• Significant memory/compute footprint (e.g. 70B model = 140+ GB RAM)<br>• Might not match latest proprietary models in quality (needs evaluation)                  |
| **Hosted LLM API**<br>(e.g. OpenAI GPT-4, Groq Cloud)               | • Best-in-class performance out-of-the-box (high answer accuracy)<br>• No infrastructure to manage; simple API integration speeds up development<br>• Scales on demand; pay-per-use model means no cost if usage is low | • Ongoing usage cost can be high for heavy usage<br>• Query data is sent to third-party (potential compliance/privacy issues)<br>• Limited ability to tailor the model beyond prompt instructions (vendor-controlled updates) |

In practice, a hybrid approach is possible. For instance, we could start with a hosted model (for rapid prototyping and higher answer quality initially). This allows quick validation of the RAG system. Once the pipeline is proven, we might **transition to an open-source model** in-house if ongoing usage makes it cost-effective or if data confidentiality becomes paramount. Services like **Groq** provide a middle ground – hosting open models like Llama-70B on optimized hardware at a lower token cost than OpenAI, which might give us the quality of a large model without managing it ourselves. We will weigh these options with usage statistics and privacy requirements. At this stage, if the chatbot is internal-only, we lean toward using an open-source model (Mistral 7B or LLaMA2 13B) served locally, to avoid external data sharing. We will carefully evaluate its accuracy; if it falls short, we can consider GPT-4 via API or a larger open model via Groq for better performance.

### Ingestion Pipeline Design

This component handles taking all the various **data sources** (PDFs, text, etc.) and converting them into vector embeddings in the chosen database. We want an automated ETL (extract-transform-load) pipeline for unstructured data. Key steps in the pipeline:

1. **Extract (Load files):** Connect to each data source and extract content:

   * PDFs (manuals, blog posts) – use a parser to get text. We can use **Unstructured** or PDF-specific loaders (e.g. PyMuPDF via LangChain). These can preserve text structure, images (with OCR if needed), etc.
   * HTML/Website pages – use an HTML loader or convert to PDF then parse.
   * Zendesk JSON – script to iterate through the exported articles and pull the relevant fields (title, body).
   * Video transcripts – they are already text, but ensure to clean any timestamps or metadata.
   * Each file or record will be represented as a **Document object** with metadata (source, title, etc.).
2. **Split (Chunking):** Split documents into bite-sized pieces for embedding. We prefer semantic chunking (by paragraph or section) rather than arbitrary fixed size, to preserve context. Tools like **LlamaIndex’s SentenceSplitter** or LangChain’s text splitter will be used (e.g. splitting by \~200-500 tokens with some overlap). This step is crucial as it impacts retrieval granularity. Since we cannot manually label sections, we rely on these automated chunkers. We will also attach metadata (document name, section headers if available) to each chunk for context.
3. **Embed:** Convert each chunk of text into a numeric vector embedding using an **embedding model**. We may use a local embedding model (e.g. Instructor XLNet or SentenceTransformers) to avoid external calls, or an API like OpenAI’s text-embedding-ada-002 if acceptable. The embedding model should be chosen for quality in the financial/trading domain if possible. This results in vectors typically of dimension 384–1536, capturing the semantic content of each chunk.
4. **Store:** Upsert the vectors (and metadata) into the vector database. This builds the index that the retriever will later query. All chunks across all sources are stored together; we can use metadata filters to scope queries (e.g. only manuals, or only knowledge base, if needed).

According to industry practice, *“data ingestion is the process of loading external documents and storing them in a vector database for retrieval,”* encompassing loading, splitting, embedding, and storing. Each of these steps will be implemented with robust libraries to minimize custom code. For example, **LlamaIndex** provides an `IngestionPipeline` abstraction that can chain these steps and even directly insert into a vector store. Using LlamaIndex or LangChain, we can leverage pre-built connectors (such as PDF loaders, HTML loaders, and the ability to write to Qdrant/Pinecone) instead of writing from scratch. This significantly speeds up development.

&#x20;*Document ingestion and retrieval pipeline: First, documents from various sources are preprocessed and chunked (using tools like LlamaIndex), then embedded into a vector database. At query time, the user’s question is also embedded and used to retrieve relevant chunks, which are passed to the LLM to generate the final answer.*

Some ingestion tooling highlights:

* **Unstructured.io** – offers connectors for many file types and can perform parsing, cleaning (removing boilerplate), and even chunking. It can handle images (via OCR) and tables, outputting content in JSON or Markdown. This could be useful for the product manuals if they contain complex formatting. Unstructured provides a *“full range of partitioning, chunking, embedding, and enrichment options”* via its API.
* **LlamaIndex & LangChain** – provide high-level frameworks. For instance, LlamaIndex can ingest a set of documents and automatically chunk and index them. It also interfaces with vector stores (we can automatically insert nodes into a Qdrant or Pinecone index through LlamaIndex configurations). LangChain provides similar functionality with its document loaders and `VectorStore` wrappers.
* **Metadata handling:** We will tag each chunk with source information (e.g. `source: "manuals"` or a document ID). This will help in debugging and also enable source-specific queries (e.g. restrict to knowledge base articles). It also allows the chatbot to cite sources in answers if desired (for internal use we may not need formal citations, but it is a nice feature to build trust – RAG enables the system to provide citations for transparency).

The ingestion pipeline will be run initially to build the knowledge index. After that, we should plan for updates: e.g. a new product manual release should be parsed and added. We can schedule periodic re-ingestion or build a small admin tool to add documents on demand. Since content overlap exists, if duplicates cause confusion, we might implement a deduplication step (e.g. hashing chunk text to skip exact duplicates). However, initially we may not worry, as the retriever’s relevancy scoring should naturally surface one of the duplicates.

### Retrieval and Orchestration

Once data is indexed, the **retrieval augmented generation workflow** at query time works as follows:

1. The user enters a question (either through a chat UI or API call).
2. The system embeds the user’s query into a vector (using the same embedding model as the documents).
3. The vector database is queried for the top *k* most similar chunks (we’ll start with k≈3–5). We may filter by source if the user’s context implies a certain source (or we might always search all).
4. The retrieved text chunks are concatenated into a context prompt. We will likely format this as a system or user prompt to the LLM, along the lines of: *“Use the following information to answer the question. Information: \[chunk1 text] \[chunk2 text] ... Question: \[user’s question]”*. We will also include a system instruction that if the answer is not contained in the provided context, the bot should say it does not know or provide a fallback (rather than guessing).
5. The LLM generates an answer using both its trained knowledge and (primarily) the provided context. If our prompt is set up correctly, it will ground the answer in the retrieved data. This **greatly reduces hallucinations** – the model isn’t left to its own devices, it has reference text.
6. The answer is returned to the user. If needed, we can also return which document sources were used (for internal users, this is useful to verify the answer; for external, we might surface brief citations or links for transparency).

We will implement the above using either **LangChain chains** or LlamaIndex’s query engine. Both allow constructing the retrieval step followed by LLM call easily. LangChain’s `RetrievalQA` chain, for example, can take a vector store and an LLM and handle the workflow end-to-end.

## Recommendation and Implementation Plan

After evaluating the options, **the recommended stack** for our use-case is:

* **Vector Database:** **Qdrant (self-hosted)** – It offers a good balance of performance and cost-efficiency. We can start with a single Docker container (easy deployment) and scale to a cluster if needed. Qdrant’s filtering and hybrid search capabilities will handle our metadata and keyword constraints well. We avoid Pinecone’s recurring costs while our scale is manageable, and keep data internal. (Alternative: Weaviate if we need built-in vectorization modules, but since we plan a separate embedding step, Qdrant suffices).
* **LLM:** **Open-Source 13B-class model** (e.g. LLaMA-2 13B or Mistral 7B) running on an internal server via the **Ollama** runtime or similar. This gives us data privacy and zero token costs. We will start with this to support internal usage. We expect that with relevant retrieved context, even a smaller model can produce correct answers, as it mostly needs to relay information from the documents. We will thoroughly test the accuracy. If we find the model struggling (especially on complex phrasing or if we open the chatbot to customers with higher expectations), we will consider switching to a **hosted model**. One pragmatic approach is to use an environment toggle: e.g., use OpenAI GPT-4 in development (to quickly see ideal performance), and use the open model in production initially; if the open model’s quality is insufficient, we can then evaluate bringing GPT-4 (or a larger open model on Groq Cloud) into production. This phased approach ensures we balance cost and accuracy.
* **Ingestion Pipeline:** Use **LlamaIndex** to orchestrate ingestion. LlamaIndex can intake our PDFs, text, and JSON and unify chunking and indexing. We will utilize **Unstructured** under the hood for PDF processing if needed (LlamaIndex can be configured to use Unstructured’s partitioning for PDFs). The output of ingestion will be stored in Qdrant via LlamaIndex’s vector store connector. This pipeline will be containerized so we can run it as a batch job whenever content updates. We will also maintain the raw parsed data (perhaps as JSON files) for audit and potential re-processing.
* **Frontend Interface:** For internal use, a simple web chat UI or Slack bot can be built. We can use an existing chat UI template that calls our backend API. The backend will host the LangChain/LlamaIndex QA chain. If external (customer-facing), we might integrate it into our support portal or website chat widget, with appropriate disclaimers.
* **System Prompt & Fallbacks:** We will craft a clear system prompt for the LLM: instructing it to **only use provided information** and not to guess. For example: *“You are a helpful assistant for TradingCo. Answer the question using **only** the content provided. If the answer is not in the provided content, say you do not have that information.”* This will be prepended to every query. This strategy ensures the model does not wander off-topic. The advantage of RAG is that the model can “admit ignorance” when context is missing – we will make that the expected behavior. In practice, we will implement a check: if the retrieval returns very low similarity scores or essentially no content, we can either (a) return a polite “I’m sorry, I don’t have that information” or (b) fall back to a secondary strategy (for instance, a broader search through a different index or using a more powerful model to see if it can answer from its own knowledge). As a simple initial fallback, we’ll likely choose the safe option of acknowledging lack of info. This corresponds to the **True Negative** scenario in evaluation – it’s better the bot says it cannot find an answer (when appropriate) than to hallucinate. We will also log these cases to continually improve the knowledge base coverage.
* **Deployment Considerations:** All components will be containerized (Docker): the Qdrant DB, the LLM server (if using Ollama or similar, this runs the model), and an API server (Python FastAPI) that implements the chat logic. This microservice architecture ensures we can scale pieces independently. For example, if many simultaneous queries occur, we might need multiple LLM containers behind a load balancer (or use a model serving solution that supports concurrency). For now, since internal usage is limited, a single instance should suffice.

**Development Plan & Timeline:** We propose an iterative, milestone-based approach:

1. **Data Audit (Week 1):** Compile all data sources. Verify we have access to latest manuals, export Zendesk articles, etc. Identify any format challenges (e.g. extremely large PDFs or media that need OCR). Define the metadata schema for documents (fields like source, date, etc.).
2. **Pipeline Implementation (Weeks 2-3):** Set up the ingestion pipeline. Parse a representative subset of documents and index them into the vector DB. This includes configuring chunk size, choosing the embedding model, and standing up the Qdrant instance. By end of Week 3, we should have a working index and be able to run test queries against it (manually).
3. **Backend Q\&A Module (Week 4):** Implement the retrieval + generation chain. Start with OpenAI GPT-4 as the generator for quick iteration. Develop the prompt template and test a handful of questions (covering each source type to ensure retrieval brings relevant text). Evaluate answer correctness. Adjust chunk sizes or number of retrieved documents if needed. Also, implement the confidence/fallback logic (e.g. if top similarity < threshold, handle as “no answer”).
4. **Model Integration (Week 5):** Swap in the chosen open-source LLM. Set up the model server (e.g. load the model with 4-bit quantization to reduce memory if needed). Test the end-to-end chatbot with this model. Compare outputs to the GPT-4 outputs from prior step to gauge quality. If quality is acceptable, proceed; if not, consider tweaks (like a larger model or hybrid approach as discussed).
5. **Frontend & Interface (Week 6):** Develop a simple UI or integrate with Slack/Teams. This involves creating an API endpoint for chat and building a minimal UI to send questions and display answers. For internal testing, this could even be a command-line or Jupyter interface initially, but ultimately a user-friendly interface is needed for support team adoption.
6. **Testing & Tuning (Weeks 7-8):** Rigorously test the chatbot with sample queries from support tickets and known customer questions. Involve product experts to validate answers. We will use metrics like accuracy and coverage of correct sources. Any mistakes (false positives or negatives) will be analyzed: if the retrieval missed something, we might need to adjust embeddings or add alternative phrasing in documents; if the LLM made something up, we will refine the prompt further or shorten the context. We will also test performance (latency) and concurrency limits. A confusion matrix evaluation can be done to categorize outcomes – to ensure the system appropriately handles queries it knows vs doesn’t know.
7. **Deployment (Week 9):** Deploy the chatbot for internal use. This includes setting up monitoring (for example, log each query and whether it was answered from the knowledge base, to identify gaps). Ensure we have alerts if any component fails (e.g. vector DB down or LLM service down). Document usage instructions for the team.
8. **Feedback and Iteration (Ongoing):** Gather user feedback from support staff. Identify frequently asked questions that might not be answered well and improve the content or add those Q\&As explicitly to the knowledge base. Plan the ingestion of **resolved support tickets** via the API as a Phase 2 (this will further enrich the knowledge base with real Q\&A pairs).

The above timeline is approximate and may be adjusted as needed. If we choose a managed component (like OpenAI API) initially, that could accelerate early phases (since no need to set up LLM server), possibly shifting more time to testing and prompt tuning.

